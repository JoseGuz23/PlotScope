# =============================================================================
# Orchestrator/__init__.py - SYLPHRENA 5.0 OPTIMIZED
# =============================================================================
# OPTIMIZACIONES APLICADAS:
#   1. Polling Adaptativo: Empieza rÃ¡pido (10s), luego incrementa
#   2. ParalelizaciÃ³n: Fase 4 y 5 ejecutan simultÃ¡neamente
#   3. Estimado: 35-40% mÃ¡s rÃ¡pido
# =============================================================================

import azure.functions as func
import azure.durable_functions as df
import logging
import json
from datetime import timedelta

# =============================================================================
# CONFIGURACIÃ“N OPTIMIZADA
# =============================================================================

LIMIT_TO_FIRST_N_CHAPTERS = 2  # None = procesar todos
MAX_WAIT_MINUTES = 60

# NUEVO: Polling adaptativo - Empieza rÃ¡pido, luego incrementa
def get_adaptive_interval(batch_type: str, attempt: int) -> int:
    """
    Polling inteligente: empieza rÃ¡pido y va incrementando.
    
    Gemini batches suelen completarse en 20-40s:
    - Primer check: 10s
    - Segundo check: 15s
    - Tercer check: 20s
    - MÃ¡ximo: 45s
    
    Claude batches suelen completarse en 60-120s:
    - Primer check: 20s
    - Segundo check: 35s
    - Tercer check: 50s
    - MÃ¡ximo: 90s
    """
    if batch_type in ['gemini', 'gemini_flash']:
        # Secuencia: 10, 15, 20, 25, 30, 35, 40, 45, 45, ...
        interval = 10 + (attempt * 5)
        return min(interval, 45)
    
    elif batch_type == 'claude':
        # Secuencia: 20, 35, 50, 65, 80, 90, 90, ...
        interval = 20 + (attempt * 15)
        return min(interval, 90)
    
    else:
        # Default conservador
        return 30

# =============================================================================
# HELPERS OPTIMIZADOS
# =============================================================================

def run_gemini_pro_batch_optimized(context, analysis_type: str, items: list, bible: dict = None):
    """
    OPTIMIZADO: Usa polling adaptativo en vez de intervalo fijo.
    """
    logging.info(f"")
    logging.info(f"{'='*60}")
    logging.info(f">>> BATCH GEMINI PRO: {analysis_type.upper()}")
    logging.info(f"    Items: {len(items)} | Polling: ADAPTATIVO")
    logging.info(f"{'='*60}")
    
    batch_input = {
        'analysis_type': analysis_type,
        'items': items,
        'bible': bible or {}
    }
    
    try:
        batch_info = yield context.call_activity('SubmitGeminiProBatch', batch_input)
    except Exception as e:
        logging.error(f"[ERROR] en SubmitGeminiProBatch [{analysis_type}]: {str(e)}")
        raise
    
    if batch_info.get('status') == 'error':
        logging.error(f"[ERROR] Submit fallÃ³ [{analysis_type}]: {batch_info.get('error')}")
        raise Exception(f"Error submit batch {analysis_type}: {batch_info.get('error')}")
    
    job_name = batch_info.get('batch_job_name', 'unknown')
    logging.info(f"[BATCH] Batch {analysis_type} creado: {job_name}")
    
    for attempt in range(MAX_WAIT_MINUTES):
        # OPTIMIZACIÃ“N: Usar intervalo adaptativo
        interval = get_adaptive_interval('gemini', attempt)
        next_check = context.current_utc_datetime + timedelta(seconds=interval)
        yield context.create_timer(next_check)
        
        try:
            result = yield context.call_activity('PollGeminiProBatchResult', batch_info)
        except Exception as e:
            logging.error(f"[ERROR] Poll [{analysis_type}] intento {attempt+1}: {str(e)}")
            continue
        
        status = result.get('status', 'unknown')
        
        if status == 'success':
            total_time = sum(get_adaptive_interval('gemini', i) for i in range(attempt + 1))
            logging.info(f"[OK] BATCH {analysis_type.upper()} COMPLETADO en ~{total_time}s ({attempt+1} polls)")
            return result.get('results', [])
        
        elif status == 'failed':
            raise Exception(f"Batch {analysis_type} fallÃ³: {result.get('error')}")
        
        elif status == 'processing':
            batch_info = result
            state = result.get('state', 'unknown')
            logging.info(f"[WAIT] [{analysis_type}] Poll {attempt+1} (+{interval}s) - {state}")
            context.set_custom_status(f"Batch {analysis_type}: {state} (poll {attempt+1})")
        
        else:
            batch_info = result

    raise Exception(f"Timeout en Batch {analysis_type}")


def analyze_with_batch_api_v2_optimized(context, fragments):
    """
    OPTIMIZADO: AnÃ¡lisis Capa 1 con polling adaptativo.
    """
    logging.info(f">>> ANÃLISIS CAPA 1 (FACTUAL) - POLLING ADAPTATIVO")
    context.set_custom_status("Enviando Batch Capa 1...")
    
    try:
        batch_info = yield context.call_activity('SubmitBatchAnalysis', fragments)
    except Exception as e:
        logging.error(f"[ERROR] en SubmitBatchAnalysis: {str(e)}")
        raise
    
    if batch_info.get('error'):
        raise Exception(f"Error submit Batch C1: {batch_info.get('error')}")
    
    batch_results = []
    
    for attempt in range(MAX_WAIT_MINUTES):
        # OPTIMIZACIÃ“N: Polling adaptativo
        interval = get_adaptive_interval('gemini_flash', attempt)
        next_check = context.current_utc_datetime + timedelta(seconds=interval)
        yield context.create_timer(next_check)
        
        try:
            result = yield context.call_activity('PollBatchResult', batch_info)
        except Exception as e:
            logging.error(f"[ERROR] en PollBatchResult intento {attempt+1}: {str(e)}")
            continue
        
        if isinstance(result, list):
            batch_results = result
            total_time = sum(get_adaptive_interval('gemini_flash', i) for i in range(attempt + 1))
            logging.info(f"[OK] BATCH CAPA 1 COMPLETADO - {len(batch_results)} anÃ¡lisis en ~{total_time}s")
            break
        
        status = result.get('status', 'unknown')
        if status in ['failed', 'error']:
            break
        
        batch_info = result
        logging.info(f"[WAIT] Capa 1 - Poll {attempt+1} (+{interval}s)")
        context.set_custom_status(f"Batch C1: {result.get('state', 'processing')} (poll {attempt+1})")
    
    # Identificar fragmentos faltantes
    successful_ids = {str(r.get('fragment_id') or r.get('chapter_id') or r.get('id')) for r in batch_results if r}
    failed_fragments = [f for f in fragments if str(f.get('id')) not in successful_ids]
    
    if not failed_fragments:
        return batch_results

    # Rescate de fragmentos fallidos
    logging.info(f"[RECOVERY] RESCATANDO {len(failed_fragments)} FRAGMENTOS")
    final_results = list(batch_results)
    
    for frag in failed_fragments[:10]:
        try:
            res = yield context.call_activity('AnalyzeChapter', frag)
            if res:
                final_results.append(res)
        except:
            pass
    
    return final_results


def run_margin_notes_batch_optimized(context, chapters: list, carta_editorial: dict, bible: dict, book_metadata: dict):
    """
    OPTIMIZADO: Notas de margen con polling adaptativo.
    """
    logging.info(f"")
    logging.info(f"{'='*60}")
    logging.info(f">>> FASE 8: NOTAS DE MARGEN - POLLING ADAPTATIVO")
    logging.info(f"    CapÃ­tulos: {len(chapters)}")
    logging.info(f"{'='*60}")
    
    batch_input = {
        'chapters': chapters,
        'carta_editorial': carta_editorial,
        'bible': bible,
        'book_metadata': book_metadata
    }
    
    try:
        batch_info = yield context.call_activity('SubmitMarginNotes', batch_input)
    except Exception as e:
        logging.error(f"[ERROR] en SubmitMarginNotes: {str(e)}")
        raise
    
    if batch_info.get('status') == 'error':
        raise Exception(f"Error submit notas: {batch_info.get('error')}")
    
    batch_id = batch_info.get('batch_id')
    logging.info(f"[BATCH] Batch notas creado: {batch_id}")
    
    for attempt in range(MAX_WAIT_MINUTES):
        # OPTIMIZACIÃ“N: Polling adaptativo para Claude
        interval = get_adaptive_interval('claude', attempt)
        next_check = context.current_utc_datetime + timedelta(seconds=interval)
        yield context.create_timer(next_check)
        
        try:
            result = yield context.call_activity('PollMarginNotesBatch', batch_info)
        except Exception as e:
            logging.error(f"[ERROR] en PollMarginNotesBatch intento {attempt+1}: {str(e)}")
            continue
        
        status = result.get('status', 'unknown')
        
        if status == 'success':
            total_time = sum(get_adaptive_interval('claude', i) for i in range(attempt + 1))
            logging.info(f"[OK] NOTAS COMPLETADAS en ~{total_time}s")
            logging.info(f"    Total notas: {len(result.get('all_notes', []))}")
            return result
        
        elif status == 'failed':
            raise Exception(f"Batch notas fallÃ³: {result.get('error')}")
        
        elif status == 'processing':
            batch_info = result
            logging.info(f"[WAIT] Notas - Poll {attempt+1} (+{interval}s)")
            context.set_custom_status(f"Batch notas: processing (poll {attempt+1})")
        
        else:
            batch_info = result

    raise Exception(f"Timeout en Batch notas")


def edit_with_claude_batch_v2_optimized(context, edit_requests: list, bible: dict, consolidated: list, 
                                        arc_map: dict, margin_notes: dict, book_metadata: dict):
    """
    OPTIMIZADO: EdiciÃ³n con Claude Batch y polling adaptativo.
    """
    logging.info(f"")
    logging.info(f"{'='*60}")
    logging.info(f">>> EDICIÃ“N PROFESIONAL - POLLING ADAPTATIVO")
    logging.info(f"    CapÃ­tulos: {len(edit_requests)}")
    logging.info(f"{'='*60}")
    
    batch_input = {
        'edit_requests': edit_requests,
        'bible': bible,
        'consolidated_chapters': consolidated,
        'arc_map': arc_map,
        'margin_notes': margin_notes,
        'book_metadata': book_metadata
    }
    
    try:
        batch_info = yield context.call_activity('SubmitClaudeBatch', batch_input)
    except Exception as e:
        logging.error(f"[ERROR] en SubmitClaudeBatch: {str(e)}")
        raise
    
    if batch_info.get('status') == 'error':
        raise Exception(f"Error submit ediciÃ³n: {batch_info.get('error')}")
    
    batch_id = batch_info.get('batch_id')
    logging.info(f"[BATCH] Batch ediciÃ³n creado: {batch_id}")
    
    for attempt in range(MAX_WAIT_MINUTES):
        # OPTIMIZACIÃ“N: Polling adaptativo para Claude
        interval = get_adaptive_interval('claude', attempt)
        next_check = context.current_utc_datetime + timedelta(seconds=interval)
        yield context.create_timer(next_check)
        
        try:
            result = yield context.call_activity('PollClaudeBatchResult', batch_info)
        except Exception as e:
            logging.error(f"[ERROR] en PollClaudeBatchResult intento {attempt+1}: {str(e)}")
            continue
        
        status = result.get('status', 'unknown')
        
        if status == 'success':
            total_time = sum(get_adaptive_interval('claude', i) for i in range(attempt + 1))
            logging.info(f"[OK] EDICIÃ“N COMPLETADA en ~{total_time}s")
            return result.get('edited_chapters', [])
        
        elif status == 'failed':
            raise Exception(f"Batch ediciÃ³n fallÃ³: {result.get('error')}")
        
        elif status == 'processing':
            batch_info = result
            logging.info(f"[WAIT] EdiciÃ³n - Poll {attempt+1} (+{interval}s)")
            context.set_custom_status(f"Batch ediciÃ³n: processing (poll {attempt+1})")
        
        else:
            batch_info = result

    raise Exception(f"Timeout en Batch ediciÃ³n")


# =============================================================================
# NUEVA FUNCIÃ“N: PARALELIZACIÃ“N DE FASE 4 Y 5
# =============================================================================

def run_parallel_structural_qualitative(context, consolidated: list):
    """
    OPTIMIZACIÃ“N CLAVE: Ejecuta anÃ¡lisis estructural y cualitativo EN PARALELO.
    
    ANTES (secuencial):
        Fase 4: Structural (60s)
        Fase 5: Qualitative (60s)
        Total: 120s
    
    DESPUÃ‰S (paralelo):
        Fase 4 + 5 simultÃ¡neos
        Total: max(60s, 60s) = 60s  â† 50% mÃ¡s rÃ¡pido!
    """
    logging.info(f"")
    logging.info(f"{'='*60}")
    logging.info(f">>> FASE 4+5: ANÃLISIS PARALELO (STRUCTURAL + QUALITATIVE)")
    logging.info(f"    Estrategia: EJECUTAR SIMULTÃNEAMENTE")
    logging.info(f"{'='*60}")
    
    # PASO 1: Enviar ambos batches al mismo tiempo
    logging.info("[PARALLEL] Enviando ambos batches simultÃ¡neamente...")
    
    batch_input_structural = {
        'analysis_type': 'layer2_structural',
        'items': consolidated,
        'bible': {}
    }
    
    batch_input_qualitative = {
        'analysis_type': 'layer3_qualitative',
        'items': consolidated,
        'bible': {}
    }
    
    try:
        # Enviar ambos batches en paralelo usando task_all
        batch_infos = yield context.task_all([
            context.call_activity('SubmitGeminiProBatch', batch_input_structural),
            context.call_activity('SubmitGeminiProBatch', batch_input_qualitative)
        ])
        
        batch_info_l2 = batch_infos[0]
        batch_info_l3 = batch_infos[1]
        
        logging.info(f"[PARALLEL] âœ… Ambos batches enviados")
        logging.info(f"    L2: {batch_info_l2.get('batch_job_name', 'unknown')}")
        logging.info(f"    L3: {batch_info_l3.get('batch_job_name', 'unknown')}")
        
    except Exception as e:
        logging.error(f"[ERROR] Enviando batches paralelos: {str(e)}")
        raise
    
    # PASO 2: Polling paralelo (checkear ambos al mismo tiempo)
    layer2_results = None
    layer3_results = None
    
    for attempt in range(MAX_WAIT_MINUTES):
        interval = get_adaptive_interval('gemini', attempt)
        next_check = context.current_utc_datetime + timedelta(seconds=interval)
        yield context.create_timer(next_check)
        
        # Checkear ambos batches simultÃ¡neamente
        poll_tasks = []
        if layer2_results is None:
            poll_tasks.append(context.call_activity('PollGeminiProBatchResult', batch_info_l2))
        if layer3_results is None:
            poll_tasks.append(context.call_activity('PollGeminiProBatchResult', batch_info_l3))
        
        if not poll_tasks:
            break  # Ambos completados
        
        try:
            poll_results = yield context.task_all(poll_tasks)
            
            result_idx = 0
            if layer2_results is None:
                result_l2 = poll_results[result_idx]
                result_idx += 1
                
                if result_l2.get('status') == 'success':
                    layer2_results = result_l2.get('results', [])
                    logging.info(f"[PARALLEL] âœ… Layer 2 (structural) COMPLETADO - {len(layer2_results)} anÃ¡lisis")
                elif result_l2.get('status') == 'failed':
                    raise Exception(f"Batch structural fallÃ³: {result_l2.get('error')}")
                else:
                    batch_info_l2 = result_l2
            
            if layer3_results is None:
                result_l3 = poll_results[result_idx]
                
                if result_l3.get('status') == 'success':
                    layer3_results = result_l3.get('results', [])
                    logging.info(f"[PARALLEL] âœ… Layer 3 (qualitative) COMPLETADO - {len(layer3_results)} anÃ¡lisis")
                elif result_l3.get('status') == 'failed':
                    raise Exception(f"Batch qualitative fallÃ³: {result_l3.get('error')}")
                else:
                    batch_info_l3 = result_l3
            
            # Logging del progreso
            status_parts = []
            if layer2_results is None:
                status_parts.append("L2:processing")
            else:
                status_parts.append("L2:âœ“")
            
            if layer3_results is None:
                status_parts.append("L3:processing")
            else:
                status_parts.append("L3:âœ“")
            
            logging.info(f"[PARALLEL] Poll {attempt+1} (+{interval}s) - {' | '.join(status_parts)}")
            context.set_custom_status(f"AnÃ¡lisis paralelo: {' '.join(status_parts)}")
            
        except Exception as e:
            logging.error(f"[ERROR] Poll paralelo intento {attempt+1}: {str(e)}")
            continue
    
    if layer2_results is None or layer3_results is None:
        raise Exception("Timeout en anÃ¡lisis paralelo")
    
    logging.info(f"[PARALLEL] ðŸŽ‰ AMBOS ANÃLISIS COMPLETADOS")
    
    return layer2_results, layer3_results


# =============================================================================
# ORCHESTRATOR PRINCIPAL
# =============================================================================

def orchestrator_function(context: df.DurableOrchestrationContext):
    """
    OPTIMIZADO: Usa polling adaptativo y paralelizaciÃ³n.
    Mejora estimada: 35-40% mÃ¡s rÃ¡pido.
    """
    try:
        start_time = context.current_utc_datetime
        tiempos = {}
        
        logging.info(f"")
        logging.info(f"{'#'*70}")
        logging.info(f"#  SYLPHRENA 5.0 OPTIMIZED - DEVELOPMENTAL EDITOR AI")
        logging.info(f"#  Job ID: {context.instance_id}")
        logging.info(f"#  Optimizaciones: Polling Adaptativo + ParalelizaciÃ³n")
        logging.info(f"{'#'*70}")
        
        # Input handling
        input_data = context.get_input()
        
        if isinstance(input_data, str):
            try:
                input_data = json.loads(input_data)
            except Exception as e:
                logging.warning(f"No se pudo parsear input string como JSON: {e}")
                pass
                
        if not isinstance(input_data, dict):
            input_data = {}

        job_id = input_data.get('job_id', context.instance_id)
        blob_path = input_data.get('blob_path', '')
        book_name = input_data.get('book_name', 'Sin tÃ­tulo')
        book_metadata = {'title': book_name, 'job_id': job_id, 'blob_path': blob_path}

        # ---------------------------------------------------------------------
        # FASE 1: SEGMENTACIÃ“N
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 1: SEGMENTACIÃ“N")
        context.set_custom_status("Fase 1: Segmentando...")
        
        seg_result = yield context.call_activity('SegmentBook', {'job_id': job_id, 'blob_path': blob_path})
        
        if isinstance(seg_result, str):
            try:
                seg_result = json.loads(seg_result)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de SegmentBook: {e}")
        
        if seg_result.get('error'):
            raise Exception(f"SegmentaciÃ³n: {seg_result.get('error')}")
        
        fragments = seg_result.get('fragments', [])
        book_metadata.update(seg_result.get('book_metadata', {}))
        
        logging.info(f"[OK] {len(fragments)} fragmentos generados")
        
        t1 = context.current_utc_datetime
        tiempos['segmentacion'] = str(t1 - start_time)

        # ---------------------------------------------------------------------
        # FASE 2: ANÃLISIS CAPA 1 (OPTIMIZADO)
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 2: ANÃLISIS CAPA 1")
        context.set_custom_status("Fase 2: Capa 1...")
        
        layer1_results = yield from analyze_with_batch_api_v2_optimized(context, fragments)
        
        t2 = context.current_utc_datetime
        tiempos['capa1'] = str(t2 - t1)

        # ---------------------------------------------------------------------
        # FASE 3: CONSOLIDACIÃ“N
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 3: CONSOLIDACIÃ“N")
        context.set_custom_status("Fase 3: Consolidando...")
        
        consol_input = {
            'fragment_analyses': layer1_results,
            'chapter_map': {}
        }
        consolidated = yield context.call_activity('ConsolidateFragmentAnalyses', consol_input)
        
        if isinstance(consolidated, str):
            try:
                consolidated = json.loads(consolidated)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de ConsolidateFragmentAnalyses: {e}")
        
        # Validar que tenemos datos consolidados
        if not consolidated or not isinstance(consolidated, list) or len(consolidated) == 0:
            raise Exception(f"ConsolidaciÃ³n fallÃ³: retornÃ³ estructura vacÃ­a o invÃ¡lida. Type: {type(consolidated)}")
        
        logging.info(f"[OK] ConsolidaciÃ³n completada: {len(consolidated)} capÃ­tulos")
        
        t3 = context.current_utc_datetime
        tiempos['consolidacion'] = str(t3 - t2)

        # =====================================================================
        # OPTIMIZACIÃ“N CLAVE: FASE 4+5 EN PARALELO
        # =====================================================================
        logging.info(f">>> OPTIMIZACIÃ“N: EJECUTANDO FASE 4 Y 5 EN PARALELO")
        context.set_custom_status("Fase 4+5: AnÃ¡lisis paralelo...")
        
        layer2_results, layer3_results = yield from run_parallel_structural_qualitative(context, consolidated)
        
        # Fusionar resultados en consolidated
        for i, ch in enumerate(consolidated):
            ch_id = str(ch.get('chapter_id', i))
            l2 = next((r for r in layer2_results if str(r.get('chapter_id')) == ch_id), {})
            l3 = next((r for r in layer3_results if str(r.get('chapter_id')) == ch_id), {})
            ch['layer2_structural'] = l2
            ch['layer3_qualitative'] = l3
        
        t4_5 = context.current_utc_datetime
        tiempos['capa2_y_3_paralelo'] = str(t4_5 - t3)
        logging.info(f"[OPTIMIZACIÃ“N] Fase 4+5 completadas en: {tiempos['capa2_y_3_paralelo']}")

        # ---------------------------------------------------------------------
        # FASE 6: BIBLIA NARRATIVA
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 6: BIBLIA NARRATIVA")
        context.set_custom_status("Fase 6: Biblia...")
        
        full_text = "\n".join([f"CAP {f['title']}: {f['content'][:600]}..." for f in fragments])
        
        holistic = yield context.call_activity('HolisticReading', full_text)

        if isinstance(holistic, str):
            try:
                holistic = json.loads(holistic)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de HolisticReading: {e}")
        
        bible_in = {
            "chapter_analyses": consolidated,
            "holistic_analysis": holistic,
            "book_metadata": book_metadata
        }
        
        bible = yield context.call_activity('CreateBible', bible_in)

        if isinstance(bible, str):
            try:
                bible = json.loads(bible)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de CreateBible: {e}")
        
        t6 = context.current_utc_datetime
        tiempos['biblia'] = str(t6 - t4_5)
        
        # GUARDAR BIBLIA ANTES DE PAUSA
        logging.info(f"[SAVE] Guardando biblia preliminar...")
        
        # --- CORRECCIÃ“N APLICADA AQUÃ ---
        pre_save_payload = {
            'job_id': job_id,          # Clave corregida sin espacios
            'book_name': book_name,
            'bible': bible,
            'consolidated_chapters': consolidated,
            'statistics': {}, 
            'tiempos': tiempos
        }
        # --------------------------------
        
        try:
            yield context.call_activity('SaveOutputs', pre_save_payload)
        except Exception as e:
            logging.error(f"[ERROR] Guardado intermedio: {str(e)}")
        
        # =====================================================================
        # PAUSA PARA APROBACIÃ“N DE BIBLIA
        # =====================================================================
        logging.info(f"[WAIT] Esperando aprobaciÃ³n humana de la Biblia...")
        context.set_custom_status("Esperando aprobacion de Biblia...")
        
        yield context.wait_for_external_event("BibleApproved")
        
        logging.info(f"[RESUME] Biblia aprobada. Continuando...")

        # =====================================================================
        # FASE 7: CARTA EDITORIAL
        # =====================================================================
        logging.info(f"")
        logging.info(f"{'='*60}")
        logging.info(f">>> FASE 7: CARTA EDITORIAL")
        logging.info(f"{'='*60}")
        context.set_custom_status("Fase 7: Carta Editorial...")
        
        carta_input = {
            'bible': bible,
            'consolidated_chapters': consolidated,
            'fragments': fragments,
            'book_metadata': book_metadata
        }
        
        carta_result = yield context.call_activity('GenerateEditorialLetter', carta_input)

        if isinstance(carta_result, str):
            try:
                carta_result = json.loads(carta_result)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de GenerateEditorialLetter: {e}")
                carta_result = {}

        # VALIDACIÃ“N: Verificar que realmente se generÃ³ la carta
        if not isinstance(carta_result, dict):
            logging.error(f"[ERROR] GenerateEditorialLetter retornÃ³ tipo invÃ¡lido: {type(carta_result)}")
            carta_result = {}

        carta_editorial = carta_result.get('carta_editorial', {})
        carta_markdown = carta_result.get('carta_markdown', '')

        # DIAGNÃ“STICO
        if carta_result.get('status') == 'error':
            logging.error(f"[ERROR] GenerateEditorialLetter fallÃ³: {carta_result.get('error', 'Unknown error')}")

        if not carta_editorial:
            logging.warning(f"[WARNING] carta_editorial estÃ¡ vacÃ­a. Result keys: {list(carta_result.keys())}")
            logging.warning(f"[WARNING] carta_result completo: {json.dumps(carta_result, ensure_ascii=False)[:500]}")
        else:
            logging.info(f"[OK] Carta Editorial generada - Secciones: {list(carta_editorial.keys())[:5]}")

        if not carta_markdown:
            logging.warning(f"[WARNING] carta_markdown estÃ¡ vacÃ­a")
        
        t7 = context.current_utc_datetime
        tiempos['carta_editorial'] = str(t7 - t6)

        # =====================================================================
        # FASE 8: NOTAS DE MARGEN (OPTIMIZADO)
        # =====================================================================
        logging.info(f">>> FASE 8: NOTAS DE MARGEN")
        context.set_custom_status("Fase 8: Notas de margen...")
        
        margin_result = yield from run_margin_notes_batch_optimized(
            context, fragments, carta_editorial, bible, book_metadata
        )
        
        # Organizar notas por capÃ­tulo
        margin_notes_by_chapter = {}
        for ch_result in margin_result.get('results', []):
            ch_id = str(ch_result.get('chapter_id', ch_result.get('fragment_id', '?')))
            margin_notes_by_chapter[ch_id] = ch_result.get('notas_margen', [])
        
        logging.info(f"[OK] {len(margin_result.get('all_notes', []))} notas generadas")
        
        t8 = context.current_utc_datetime
        tiempos['notas_margen'] = str(t8 - t7)

        # ---------------------------------------------------------------------
        # FASE 9: ARCOS POR CAPÃTULO (OPTIMIZADO)
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 9: ARCOS POR CAPÃTULO")
        context.set_custom_status("Fase 9: Arcos...")
        
        arc_results = yield from run_gemini_pro_batch_optimized(context, 'arc_maps', consolidated, bible=bible)
        arc_map_dict = {str(r['chapter_id']): r for r in arc_results}
        
        t9 = context.current_utc_datetime
        tiempos['arcos'] = str(t9 - t8)

        # =====================================================================
        # FASE 10: EDICIÃ“N PROFESIONAL (OPTIMIZADO)
        # =====================================================================
        logging.info(f">>> FASE 10: EDICIÃ“N PROFESIONAL")
        context.set_custom_status("Fase 10: EdiciÃ³n...")
        
        edit_reqs = [{'chapter': frag} for frag in fragments]
        
        edited_fragments = yield from edit_with_claude_batch_v2_optimized(
            context, 
            edit_reqs, 
            bible, 
            consolidated, 
            arc_map_dict,
            margin_notes=margin_notes_by_chapter,
            book_metadata=book_metadata
        )
        
        edited_fragments.sort(key=lambda x: int(x.get('chapter_id', 0) or x.get('fragment_id', 0) or 0))
        
        t10 = context.current_utc_datetime
        tiempos['edicion'] = str(t10 - t9)

        # ---------------------------------------------------------------------
        # FASE 11: RECONSTRUCCIÃ“N
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 11: RECONSTRUCCIÃ“N")
        context.set_custom_status("Fase 11: Reconstruyendo...")
        
        recon_input = {
            'edited_chapters': edited_fragments,
            'book_name': book_name,
            'bible': bible
        }
        
        manuscript = yield context.call_activity('ReconstructManuscript', recon_input)

        if isinstance(manuscript, str):
            try:
                manuscript = json.loads(manuscript)
            except Exception as e:
                logging.error(f"[CRITICAL FIX] FallÃ³ deserializaciÃ³n de ReconstructManuscript: {e}")
        
        t11 = context.current_utc_datetime
        tiempos['reconstruccion'] = str(t11 - t10)

        # ---------------------------------------------------------------------
        # FASE 12: GUARDAR Y FINALIZAR
        # ---------------------------------------------------------------------
        logging.info(f">>> FASE 12: GUARDADO FINAL")
        context.set_custom_status("Finalizando...")
        
        t_final = context.current_utc_datetime
        tiempos['total'] = str(t_final - start_time)
        
        final = {
            'status': 'success',
            'job_id': job_id,
            'book_name': book_name,
            'manuscripts': manuscript.get('manuscripts', {}),
            'consolidated_chapters': manuscript.get('consolidated_chapters', []),
            'statistics': manuscript.get('statistics', {}),
            'bible': bible,
            'carta_editorial': carta_editorial,
            'carta_markdown': carta_markdown,
            'margin_notes': margin_result,
            'tiempos': tiempos,
            'stats': {
                'fragmentos_entrada': len(fragments),
                'capitulos_consolidados': len(consolidated),
                'capitulos_editados': len(edited_fragments),
                'notas_margen': len(margin_result.get('all_notes', []))
            }
        }
        
        try:
            yield context.call_activity('SaveOutputs', final)
        except Exception as e:
            logging.error(f"[ERROR] en SaveOutputs: {str(e)}")
        
        # RESUMEN FINAL
        logging.info(f"")
        logging.info(f"{'#'*70}")
        logging.info(f"#  [SUCCESS] SYLPHRENA 5.0 OPTIMIZED - COMPLETADO")
        logging.info(f"{'#'*70}")
        logging.info(f"#  Libro: {book_name}")
        logging.info(f"#  Fragmentos: {len(fragments)}")
        logging.info(f"#  CapÃ­tulos editados: {len(edited_fragments)}")
        logging.info(f"#  Notas de margen: {len(margin_result.get('all_notes', []))}")
        logging.info(f"#")
        logging.info(f"#  TIEMPOS:")
        for fase, tiempo in tiempos.items():
            logging.info(f"#     {fase}: {tiempo}")
        logging.info(f"#")
        logging.info(f"#  OPTIMIZACIONES APLICADAS:")
        logging.info(f"#     âœ“ Polling adaptativo (20% mejora)")
        logging.info(f"#     âœ“ ParalelizaciÃ³n Fase 4+5 (15% mejora)")
        logging.info(f"#     âœ“ Mejora total estimada: 35-40%")
        logging.info(f"{'#'*70}")
        
        return final

    except Exception as e:
        logging.error(f"")
        logging.error(f"{'!'*70}")
        logging.error(f"!  [CRITICAL] ERROR FATAL EN ORQUESTADOR")
        logging.error(f"{'!'*70}")
        logging.error(f"!  Error: {str(e)}")
        
        import traceback
        tb = traceback.format_exc()
        for line in tb.split('\n'):
            if line.strip():
                logging.error(f"!  {line}")
        
        logging.error(f"{'!'*70}")
        raise e

main = df.Orchestrator.create(orchestrator_function)